<!DOCTYPE html><html><head><meta charset="utf-8"><style>body {
  width: 45em;
  border: 1px solid #ddd;
  outline: 1300px solid #fff;
  margin: 16px auto;
}

body .markdown-body
{
  padding: 30px;
}

@font-face {
  font-family: octicons-anchor;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

.markdown-body {
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #333;
  overflow: hidden;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body strong {
  font-weight: bold;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body * {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body input {
  font: 13px/1.4 Helvetica, arial, freesans, clean, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
}

.markdown-body a {
  color: #4183c4;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:focus,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before {
  display: table;
  content: "";
}

.markdown-body hr:after {
  display: table;
  clear: both;
  content: "";
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code {
  font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
  font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body .octicon {
  font: normal normal 16px octicons-anchor;
  line-height: 1;
  display: inline-block;
  text-decoration: none;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.markdown-body .octicon-link:before {
  content: '\f05c';
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body .anchor {
  position: absolute;
  top: 0;
  bottom: 0;
  left: 0;
  display: block;
  padding-right: 6px;
  padding-left: 30px;
  margin-left: -30px;
}

.markdown-body .anchor:focus {
  outline: none;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  position: relative;
  margin-top: 1em;
  margin-bottom: 16px;
  font-weight: bold;
  line-height: 1.4;
}

.markdown-body h1 .octicon-link,
.markdown-body h2 .octicon-link,
.markdown-body h3 .octicon-link,
.markdown-body h4 .octicon-link,
.markdown-body h5 .octicon-link,
.markdown-body h6 .octicon-link {
  display: none;
  color: #000;
  vertical-align: middle;
}

.markdown-body h1:hover .anchor,
.markdown-body h2:hover .anchor,
.markdown-body h3:hover .anchor,
.markdown-body h4:hover .anchor,
.markdown-body h5:hover .anchor,
.markdown-body h6:hover .anchor {
  padding-left: 8px;
  margin-left: -30px;
  line-height: 1;
  text-decoration: none;
}

.markdown-body h1:hover .anchor .octicon-link,
.markdown-body h2:hover .anchor .octicon-link,
.markdown-body h3:hover .anchor .octicon-link,
.markdown-body h4:hover .anchor .octicon-link,
.markdown-body h5:hover .anchor .octicon-link,
.markdown-body h6:hover .anchor .octicon-link {
  display: inline-block;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body code {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .highlight pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

.markdown-body .highlight {
  background: #fff;
}

.markdown-body .highlight .h {
  color: #333;
  font-style: normal;
  font-weight: normal;
}

.markdown-body .highlight .mf,
.markdown-body .highlight .mh,
.markdown-body .highlight .mi,
.markdown-body .highlight .mo,
.markdown-body .highlight .il,
.markdown-body .highlight .m {
  color: #945277;
}

.markdown-body .highlight .s,
.markdown-body .highlight .sb,
.markdown-body .highlight .sc,
.markdown-body .highlight .sd,
.markdown-body .highlight .s2,
.markdown-body .highlight .se,
.markdown-body .highlight .sh,
.markdown-body .highlight .si,
.markdown-body .highlight .sx,
.markdown-body .highlight .s1 {
  color: #df5000;
}

.markdown-body .highlight .kc,
.markdown-body .highlight .kd,
.markdown-body .highlight .kn,
.markdown-body .highlight .kp,
.markdown-body .highlight .kr,
.markdown-body .highlight .kt,
.markdown-body .highlight .k,
.markdown-body .highlight .o {
  font-weight: bold;
}

.markdown-body .highlight .kt {
  color: #458;
}

.markdown-body .highlight .c,
.markdown-body .highlight .cm,
.markdown-body .highlight .c1 {
  color: #998;
  font-style: italic;
}

.markdown-body .highlight .cp,
.markdown-body .highlight .cs,
.markdown-body .highlight .cp .h {
  color: #999;
  font-weight: bold;
}

.markdown-body .highlight .cs {
  font-style: italic;
}

.markdown-body .highlight .n {
  color: #333;
}

.markdown-body .highlight .na,
.markdown-body .highlight .nv,
.markdown-body .highlight .vc,
.markdown-body .highlight .vg,
.markdown-body .highlight .vi {
  color: #008080;
}

.markdown-body .highlight .nb {
  color: #0086B3;
}

.markdown-body .highlight .nc {
  color: #458;
  font-weight: bold;
}

.markdown-body .highlight .no {
  color: #094e99;
}

.markdown-body .highlight .ni {
  color: #800080;
}

.markdown-body .highlight .ne {
  color: #990000;
  font-weight: bold;
}

.markdown-body .highlight .nf {
  color: #945277;
  font-weight: bold;
}

.markdown-body .highlight .nn {
  color: #555;
}

.markdown-body .highlight .nt {
  color: #000080;
}

.markdown-body .highlight .err {
  color: #a61717;
  background-color: #e3d2d2;
}

.markdown-body .highlight .gd {
  color: #000;
  background-color: #fdd;
}

.markdown-body .highlight .gd .x {
  color: #000;
  background-color: #faa;
}

.markdown-body .highlight .ge {
  font-style: italic;
}

.markdown-body .highlight .gr {
  color: #aa0000;
}

.markdown-body .highlight .gh {
  color: #999;
}

.markdown-body .highlight .gi {
  color: #000;
  background-color: #dfd;
}

.markdown-body .highlight .gi .x {
  color: #000;
  background-color: #afa;
}

.markdown-body .highlight .go {
  color: #888;
}

.markdown-body .highlight .gp {
  color: #555;
}

.markdown-body .highlight .gs {
  font-weight: bold;
}

.markdown-body .highlight .gu {
  color: #800080;
  font-weight: bold;
}

.markdown-body .highlight .gt {
  color: #aa0000;
}

.markdown-body .highlight .ow {
  font-weight: bold;
}

.markdown-body .highlight .w {
  color: #bbb;
}

.markdown-body .highlight .sr {
  color: #017936;
}

.markdown-body .highlight .ss {
  color: #8b467f;
}

.markdown-body .highlight .bp {
  color: #999;
}

.markdown-body .highlight .gc {
  color: #999;
  background-color: #EAF2F5;
}

.markdown-body kbd {
  background-color: #e7e7e7;
  background-image: -webkit-linear-gradient(#fefefe, #e7e7e7);
  background-image: linear-gradient(#fefefe, #e7e7e7);
  background-repeat: repeat-x;
  display: inline-block;
  padding: 3px 5px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  line-height: 10px;
  color: #000;
  border: 1px solid #cfcfcf;
  border-radius: 2px;
}

.markdown-body .highlight .pl-coc,
.markdown-body .highlight .pl-entm,
.markdown-body .highlight .pl-eoa,
.markdown-body .highlight .pl-mai .pl-sf,
.markdown-body .highlight .pl-pdv,
.markdown-body .highlight .pl-sc,
.markdown-body .highlight .pl-sr,
.markdown-body .highlight .pl-v,
.markdown-body .highlight .pl-vpf {
  color: #0086b3;
}

.markdown-body .highlight .pl-eoac,
.markdown-body .highlight .pl-mdht,
.markdown-body .highlight .pl-mi1,
.markdown-body .highlight .pl-mri,
.markdown-body .highlight .pl-va,
.markdown-body .highlight .pl-vpu {
  color: #008080;
}

.markdown-body .highlight .pl-c,
.markdown-body .highlight .pl-pdc {
  color: #b4b7b4;
  font-style: italic;
}

.markdown-body .highlight .pl-k,
.markdown-body .highlight .pl-ko,
.markdown-body .highlight .pl-kolp,
.markdown-body .highlight .pl-mc,
.markdown-body .highlight .pl-mr,
.markdown-body .highlight .pl-ms,
.markdown-body .highlight .pl-s,
.markdown-body .highlight .pl-sok,
.markdown-body .highlight .pl-st {
  color: #6e5494;
}

.markdown-body .highlight .pl-ef,
.markdown-body .highlight .pl-enf,
.markdown-body .highlight .pl-enm,
.markdown-body .highlight .pl-entc,
.markdown-body .highlight .pl-eoi,
.markdown-body .highlight .pl-sf,
.markdown-body .highlight .pl-smc {
  color: #d12089;
}

.markdown-body .highlight .pl-ens,
.markdown-body .highlight .pl-eoai,
.markdown-body .highlight .pl-kos,
.markdown-body .highlight .pl-mh .pl-pdh,
.markdown-body .highlight .pl-mp,
.markdown-body .highlight .pl-pde,
.markdown-body .highlight .pl-stp {
  color: #458;
}

.markdown-body .highlight .pl-enti {
  color: #d12089;
  font-weight: bold;
}

.markdown-body .highlight .pl-cce,
.markdown-body .highlight .pl-enc,
.markdown-body .highlight .pl-kou,
.markdown-body .highlight .pl-mq {
  color: #f93;
}

.markdown-body .highlight .pl-mp1 .pl-sf {
  color: #458;
  font-weight: bold;
}

.markdown-body .highlight .pl-cos,
.markdown-body .highlight .pl-ent,
.markdown-body .highlight .pl-md,
.markdown-body .highlight .pl-mdhf,
.markdown-body .highlight .pl-ml,
.markdown-body .highlight .pl-pdc1,
.markdown-body .highlight .pl-pds,
.markdown-body .highlight .pl-s1,
.markdown-body .highlight .pl-scp,
.markdown-body .highlight .pl-sol {
  color: #df5000;
}

.markdown-body .highlight .pl-c1,
.markdown-body .highlight .pl-cn,
.markdown-body .highlight .pl-pse,
.markdown-body .highlight .pl-pse .pl-s2,
.markdown-body .highlight .pl-vi {
  color: #a31515;
}

.markdown-body .highlight .pl-mb,
.markdown-body .highlight .pl-pdb {
  color: #df5000;
  font-weight: bold;
}

.markdown-body .highlight .pl-mi,
.markdown-body .highlight .pl-pdi {
  color: #6e5494;
  font-style: italic;
}

.markdown-body .highlight .pl-ms1 {
  background-color: #f5f5f5;
}

.markdown-body .highlight .pl-mdh,
.markdown-body .highlight .pl-mdi {
  font-weight: bold;
}

.markdown-body .highlight .pl-mdr {
  color: #0086b3;
  font-weight: bold;
}

.markdown-body .highlight .pl-s2 {
  color: #333;
}

.markdown-body .highlight .pl-ii {
  background-color: #df5000;
  color: #fff;
}

.markdown-body .highlight .pl-ib {
  background-color: #f93;
}

.markdown-body .highlight .pl-id {
  background-color: #a31515;
  color: #fff;
}

.markdown-body .highlight .pl-iu {
  background-color: #b4b7b4;
}

.markdown-body .highlight .pl-mo {
  color: #969896;
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  float: left;
  margin: 0.3em 0 0.25em -1.6em;
  vertical-align: middle;
}</style><title>comparison_report</title></head><body><article class="markdown-body"><h1>
<a id="user-content-comparing-facial-emotion-recognition-technology" class="anchor" href="#comparing-facial-emotion-recognition-technology" aria-hidden="true"><span class="octicon octicon-link"></span></a>Comparing Facial Emotion Recognition Technology</h1>

<p>This report compares the results of our internal test of three facial emotion recognition technologies from different vendors. We had each technology analyze the same set of test videos. Next, we compared the results, looking for congruence between the technologies, for markers of instability or failure, and for consistency. We present preliminary results of this analysis, concluding that the state of the art of facial emotion recognition is currently only suited for highly controlled environments where very clear and expressive emotions are the point of detection and analysis. As such, we wouldn't at this point recommend it for the purpose of analyzing subtle changes in emotion during a psychodiagnostic test administered online, without any control over the recording conditions.</p>

<h2>
<a id="user-content-technology-vendors" class="anchor" href="#technology-vendors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Technology Vendors</h2>

<p>We compared technology from three separate vendors: <em>Emotient</em>, <em>IntraFace</em> and <em>Noldus</em>. We utilized trial versions (free minute allowance in the case of <em>Emotient</em>) from all the vendors. We've been in direct contact with all of them to a greater or lesser extent. </p>

<h3>
<a id="user-content-emotient" class="anchor" href="#emotient" aria-hidden="true"><span class="octicon octicon-link"></span></a>Emotient</h3>

<p><em>Emotient</em> is an online service for analyzing sentiment, attention and engagement in video recordings. We have used their product demo and their free allowance of 15 minutes of video to conduct our tests. We have also been in touch with <em>Emotient</em> with regards to pricing. <em>Emotient</em> is a purely cloud-based service, without the option of running the analysis on your own servers. The analysis is not real-time (it usually takes a few minutes for a short video to be analyzed). The primary reporting is visual, with the option of exporting the raw data in a text (CSV) format.</p>

<h3>
<a id="user-content-intraface" class="anchor" href="#intraface" aria-hidden="true"><span class="octicon octicon-link"></span></a>IntraFace</h3>

<p><em>IntraFace</em> is a product spun off from the <em>Carnegie Mellon University</em> in Pittsburgh, PA. It is an offline product (a C++ library) for face detection and emotion analysis. It builds on research conducted in the <em>Human Sensing</em> laboratory of the CMU's School of Computer Science. However, it is now an independent commercial product. We have been in extensive communication with the founder (Fernando De la Torre) about pricing, the future plans with the product (it is still undergoing active development). It was also technically the most difficult to set up, as we had to implement the interface and reporting tools ourselves, using the provided trial version of the library. As such, we were able to fully customize the output as a CSV file.</p>

<h3>
<a id="user-content-noldus-facereader" class="anchor" href="#noldus-facereader" aria-hidden="true"><span class="octicon octicon-link"></span></a>Noldus FaceReader</h3>

<p><em>Noldus</em>, a Dutch company, makes <em>FaceReader</em>, software designed primarily for marketers and researchers (in controlled lab settings) for analyzing emotions from facial expressions. They provide both an offline version, and an online cloud-based service. We have exchanged several emails and held a Skype call with Noldus to go over the details of implementing their solution, as well as their pricing terms. We have used the trial version of their offline software to conduct our tests. The output is primarly visual in the software itself, but it allows for export to a text (TXT) file.</p>

<h2>
<a id="user-content-methodology" class="anchor" href="#methodology" aria-hidden="true"><span class="octicon octicon-link"></span></a>Methodology</h2>

<p>We have first recorded a set of seven (7) short videos that varied in expresiveness, lighting conditions and whether the subject on camera wore glasses or not. Some were supposed to be very neutral, some very expressive, some only involved mouth movements, some incorporated eye expressions. One (called the <em>edge case</em> video) was designed to test the extremes of the technology: a very highly-moving subject, constantly changing facial expressions. The videos were recorded on a standard USB web camera from Microsoft to simulate the possible lower end of the quality spectrum of what our actual candidates may use. </p>

<p>Secondly, we also recorded a video of a subject actually taking a psychodiagnostic test online to simulate more realistically the range of emotions and movements that our candidates may exhibit during a test. This was a longer video with appropriate lighting conditions and a slightly better mid-range USB web camera. The subject wore glasses during the test as we cannot expect candidates who need glasses to take them off for a substantial amount of time where their sight cannot be impaired.</p>

<p>Then, we used the three technologies to analyze these videos. We then gathered the results and combined them into comparable spreadsheets that allowed for side-by-side analysis.</p>

<h2>
<a id="user-content-hypotheses" class="anchor" href="#hypotheses" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hypotheses</h2>

<p>Prior to going into these tests, we formulated several hypotheses that we considered critical for us to be able to use this technology in production.</p>

<ol>
<li>The results of the three technologies on an identical recorded video will not vary substantially. Specifically, the shares of the positive, negative and neutral emotions will not be significantly different.</li>
<li>The technologies are able to identify a face despite less than ideal conditions: with low lighting, when the subject wears glasses, etc. </li>
<li>The ability of the technologies to identify a face will not vary significantly across the three vendors.</li>
</ol>

<p>As we want to choose a vendor (and a technology) that we can vouch for, and the we can have confidence in that it provides reliable results to our clients, we decided that if we find no support for these hypotheses in the analysis, we would prefer not to use facial expression analysis at this point rather than use an unreliable solution.</p>

<h2>
<a id="user-content-data-preparation" class="anchor" href="#data-preparation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Preparation</h2>

<p>Before comparing the results, we needed to standarize results from the three vendors so that they can be compared side-by-side. This section describes the necessary steps we needed to take to make that possible.</p>

<h3>
<a id="user-content-differences-in-time-frames" class="anchor" href="#differences-in-time-frames" aria-hidden="true"><span class="octicon octicon-link"></span></a>Differences in time frames</h3>

<p>First problem we encountered was when comparing results from <em>Emotient</em> and <em>IntraFace</em>. In several videos, <em>IntraFace</em> results produced approximately ten times fewer entries (frames) than those of <em>Emotient</em>. The test videos were recorded at a rather low frame rate (approx. <code>4 fps</code>). However, it apperead that <em>Emotient</em> had treated the videos as standard <code>30 fps</code> videos and somehow extrapolated the frames to match this frame rate. Therefore, to be able to correlate the results and compare them graphically, we had to downsample <em>Emotient</em> results to a size that was comparable to that of <em>IntraFace</em>. </p>

<p>The heuristic used was as follows: only keep frames from the <em>Emotient</em> results, timestamp of which is sufficiently close to at least one timestamp in the <em>IntraFace</em> results. Sufficiently cňlose was defined as "the difference between the <em>Emotient</em> timestamp and the <em>IntraFace</em> timestamp is less than <code>epsilon</code>", where <code>epsilon</code> is an arbitrarily chosen small number. After a few tries, the <code>epsilon</code> parameter that most accurately downsampled the results was found to be <code>0.0007s</code> (or, more generally, between <code>0.001s</code> and <code>0.0005s</code>).</p>

<h3>
<a id="user-content-differences-in-specific-emotions" class="anchor" href="#differences-in-specific-emotions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Differences in specific emotions</h3>

<p>Each technology used uses a slightly different set of emotions to classify the video. <em>IntraFace</em> uses a set of five basic emotions: <strong>Anger</strong>, <strong>Disgust</strong>, <strong>Joy</strong>, <strong>Sadness</strong> and <strong>Surprise</strong>. <em>Emotient</em> adds <strong>Confusion</strong>, <strong>Contempt</strong> and <strong>Frustration</strong>. <em>Noldus</em> classifies <strong>Joy</strong>, <strong>Sadness</strong>, <strong>Fear</strong>, <strong>Disgust</strong>, <strong>Surprise</strong>, <strong>Anger</strong> and <strong>Contempt</strong>. All technologies also classify <strong>Neutral</strong> as a sort of a baseline emotion.</p>

<p>Given these differences, it wasn't possible to reliably compare results in individual emotions. The primary reason is this. Suppose we group all the emotions into <strong>positive</strong> and <strong>negative</strong> bins. (See table below.) Also suppose that all three classifiers agree perfectly on whether a person has a positive, negative or neutral emotion. Then, having decided that the emotion is negative, each classifier has a different set of options to assign the expression into. While <em>IntraFace</em> can only choose between <strong>Anger</strong>, <strong>Disgust</strong> and <strong>Sadness</strong>, the other two classifiers have more options. Assuming the classifiers <em>need</em> to assign some emotion to the expression, the <em>Independence of Irrelevant Alternatives</em> fails in this example and specific results from different classifiers are thus not comparable.</p>

<table>
<thead>
<tr>
<th>Emotion</th>
<th>Vendors</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td>Joy</td>
<td>all</td>
<td>Positive</td>
</tr>
<tr>
<td>Surprise</td>
<td>all</td>
<td>Positive</td>
</tr>
<tr>
<td>Anger</td>
<td>all</td>
<td>Negative</td>
</tr>
<tr>
<td>Disgust</td>
<td>all</td>
<td>Negative</td>
</tr>
<tr>
<td>Sadness</td>
<td>all</td>
<td>Negative</td>
</tr>
<tr>
<td>Confusion</td>
<td>Emotient</td>
<td>Negative</td>
</tr>
<tr>
<td>Contempt</td>
<td>Emotient and Noldus</td>
<td>Negative</td>
</tr>
<tr>
<td>Frustration</td>
<td>Emotient</td>
<td>Negative</td>
</tr>
<tr>
<td>Fear</td>
<td>Noldus</td>
<td>Negative</td>
</tr>
</tbody>
</table>

<p>As we can see, this only affects negative emotions. Nonetheless, we have decide to only compare classification into the large emotional bins of <strong>positive</strong>, <strong>negative</strong> and <strong>neutral</strong>. </p>

<p>(This is also necessary as - having only discovered this during the analysis - <em>IntraFace</em> hasn't implemented the <strong>Anger</strong> emotion yet.)</p>

<h2>
<a id="user-content-results" class="anchor" href="#results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Results</h2>

<h3>
<a id="user-content-failure-of-noldus" class="anchor" href="#failure-of-noldus" aria-hidden="true"><span class="octicon octicon-link"></span></a>Failure of Noldus</h3>

<p>Prior to comparing specific results in the different tests, one technology rejected the latter two hypotheses from the outset. <em>Noldus</em> had significant problems with detecting a face, especially when the subject wore glasses or when their head was even slightly tilted or turned. We attempted calibration, but to no avail. Given the lackluster performance in identifying a face, the results produced by Noldus were unusable and we thus decided not to use them in further analysis.</p>

<h3>
<a id="user-content-first-seven-videos" class="anchor" href="#first-seven-videos" aria-hidden="true"><span class="octicon octicon-link"></span></a>First seven videos</h3>

<p>As mentioned in <code>Data Preparation</code> above, we will limit our analysis to comparing general emotional categories: <strong>positive</strong>, <strong>negative</strong> and <strong>neutral</strong>. Overall, the shares of emotions assigned to these bins were relatively similar. The table below shows the different videos with the respective emotions assigned by <em>Emotient</em> and <em>Noldus</em>:</p>

<table>
<thead>
<tr>
<th></th>
<th>One</th>
<th>Two</th>
<th>Three</th>
<th>Four</th>
<th>Five</th>
<th>Six</th>
<th>Seven</th>
</tr>
</thead>
<tbody>
<tr>
<td>Positive Emotient</td>
<td>1.0%</td>
<td>0.4%</td>
<td>27.1%</td>
<td>27.5%</td>
<td>9.9%</td>
<td>21.5%</td>
<td>38.9%</td>
</tr>
<tr>
<td>Positive IntraFace</td>
<td>4.0%</td>
<td>0.9%</td>
<td>31.0%</td>
<td><strong>47.6%</strong></td>
<td>14.3%</td>
<td>22.5%</td>
<td>46.7%</td>
</tr>
<tr>
<td>Negative Emotient</td>
<td>21.5%</td>
<td>24.5%</td>
<td>48.1%</td>
<td>63.5%</td>
<td>50.2%</td>
<td>51.1%</td>
<td>36.4%</td>
</tr>
<tr>
<td>Negative IntraFace</td>
<td>16.9%</td>
<td>9.5%</td>
<td>30.5%</td>
<td><strong>34.6%</strong></td>
<td>46.2%</td>
<td><strong>25.6%</strong></td>
<td>20.4%</td>
</tr>
<tr>
<td>Neutral Emotient</td>
<td>77.5%</td>
<td>75.1%</td>
<td>24.8%</td>
<td>9.0%</td>
<td>39.9%</td>
<td>27.4%</td>
<td>24.7%</td>
</tr>
<tr>
<td>Neutral IntraFace</td>
<td>79.1%</td>
<td>89.6%</td>
<td>38.6%</td>
<td><strong>17.9%</strong></td>
<td>39.5%</td>
<td><strong>51.9%</strong></td>
<td>32.9%</td>
</tr>
</tbody>
</table>

<p>We can see that only in the fourth and the sixth video were there any major discrepancies between the categories. However, having no "ground truth" to compare the results with, there is only so far comparisons like these can take us. They don't allow us to ascertain the accuracy of the two classifiers, only whether they agree. And on the very broad level of positive/negative/neutral they seem to do. </p>

<p>Granted, when one looks at graphs of the positive/negative/neutral scores over time, there are apparent inconsistencies. As an example, here's the graphs depicting positive, negative and neutral emotions over time in the sixth video:</p>

<p><a href="graph6.png" target="_blank"><img src="graph6.png" alt="Graph Video 6" title="Emotions in Video 6" style="max-width:100%;"></a></p>

<p>It is clear that the two lines don't perfectly match up. Specifically, <em>IntraFace</em> appears to be more "jittery" - changing emotions more often, perhaps being more sensitive to smaller changes in expression.</p>

<h3>
<a id="user-content-test-taking-video" class="anchor" href="#test-taking-video" aria-hidden="true"><span class="octicon octicon-link"></span></a>Test taking video</h3>

<p>The second test came in form of a recording from actual questionnaire taking. A subject was recorded while taking a psychodiagnostic test on a computer. The video is approximately 8 minutes long. Just at a glance, the subject appeared rather neutral, disengaged or perhaps slightly bored when taking the test. That's why the results from <em>Emotient</em> and <em>IntraFace</em> surprised us (<em>Noldus</em> again failed to identify the face for most of the recording). Specifically, there was a large disconnect between the base results of <em>Emotient</em> and <em>IntraFace</em>:</p>

<table>
<thead>
<tr>
<th></th>
<th>Emotient</th>
<th>IntraFace</th>
</tr>
</thead>
<tbody>
<tr>
<td>Positive</td>
<td>13.7%</td>
<td>4.1%</td>
</tr>
<tr>
<td>Negative</td>
<td><strong>74.4%</strong></td>
<td>2.5%</td>
</tr>
<tr>
<td>Neutral</td>
<td>11.9%</td>
<td><strong>93.4%</strong></td>
</tr>
</tbody>
</table>

<p>While <em>Emotient</em> classified a large part of the video as negative (with <strong>Sadness</strong> and <strong>Confusion</strong> making up the vast majority of that, over 58%), <em>IntraFace</em> was overwhelmingly neutral. If anything, there were more positive than negative emotions in the <em>IntraFace</em> results (<strong>Joy</strong> was the dominant emotion with 3.2%, followed by <strong>Sadness</strong> at 2.0%). </p>

<p>Given that this was a video that was supposed to represent our standard use case, the results shocked us. The numbers themselves are of secondary concern (the subject may well have been sad and confused - or bored, which could manifest as the two former emotions), the discrepancy is the primary concern. Unlike the videos in the first test, where the discrepancies were mostly minor, this is a significant difference in identifying the dominant sentiment in the video.</p>
</article></body></html>